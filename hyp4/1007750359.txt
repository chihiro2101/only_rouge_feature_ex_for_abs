in the case of normal distribution data , the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean , and 1 in 370 will deviate by three times the standard deviation in a sample of 1000 observations , the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected , being less than twice the expected number and hence within 1 standard deviation of the expected number – see poisson distribution – and not indicate an anomaly if the sample size is only 100 , however , just three such outliers are already reason for concern , being more than 11 times the expected number in general , if the nature of the population distribution is known a priori , it is possible to test if the number of outliers deviate significantly from what can be expected : for a given cutoff ( so samples fall beyond the cutoff with probability p ) of a given distribution , the number of outliers will follow a binomial distribution with parameter p , which can generally be well-approximated by the poisson distribution with λ pn a sample may have been contaminated with elements from outside the population being examined alternatively , an outlier could be the result of a flaw in the assumed theory , calling for further investigation by the researcher additionally , the pathological appearance of outliers of a certain form appears in a variety of datasets , indicating that the causative mechanism for the data might differ at the extreme end ( king effect ) there is no rigid mathematical definition of what constitutes an outlier ; determining whether or not an observation is an outlier is ultimately a subjective exercise model-based methods which are commonly used for identification assume that the data are from a normal distribution , and identify observations which are deemed '' unlikely '' based on mean and standard deviation : chauvenet 's criterion grubbs 's test for outliers dixon 's q test astm e178 standard practice for dealing with outlying observations mahalanobis distance and leverage are often used to detect outliers , especially in the development of linear regression models john tukey proposed this test , where k 1.5 indicates an '' outlier '' , and k 3 indicates data that is '' far out '' in various domains such as , but not limited to , statistics , signal processing , finance , econometrics , manufacturing , networking and data mining , the task of anomaly detection may take other approaches if a data point ( or points ) is excluded from the data analysis , this should be clearly stated on any subsequent report in cases where the cause of the outliers is known , it may be possible to incorporate this effect into the model structure , for example by using a hierarchical bayes model , or a mixture model 