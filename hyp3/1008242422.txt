the simplest primality test is trial division : given an input number , n , check whether it is evenly divisible by any prime number between 2 and ( i.e riesel ( 1994 ) pp.2-3 for example , consider the number 100 , which is evenly divisible by these numbers : :2 , 4 , 5 , 10 , 20 , 25 , 50 note that the largest factor , 50 , is half of 100 unlike the other probabilistic tests , this algorithm produces a primality certificate , and thus can be used to prove that a number is prime a combination of shor 's algorithm , an integer factorization method , with the pocklington primality test could solve the problem in o ( ( \log n ) ^3 ( \log\log n ) ^2 \log\log\log n ) however , as this test requires a partial factorization of n & nbsp ; − & nbsp ; 1 the running time was still quite slow in the worst case ( note that running time is measured in terms of the size of the input , which in this case is ~ log & nbsp ; n , that being the number of bits needed to represent the number n similarly , under the generalized riemann hypothesis , the deterministic miller 's test , which forms the basis of the probabilistic miller–rabin test , can be proved to run in õ ( ( log & nbsp ; n ) 4 ) it is easy to show that primes is in co-np : its complement composites is in np because one can decide compositeness by nondeterministically guessing a factor because of its tractability in practice , polynomial-time algorithms assuming the riemann hypothesis , and other similar evidence , it was long suspected but not proven that primality could be solved in polynomial time these tests typically require factorization of n & nbsp ; + & nbsp ; 1 , n − 1 , or a similar quantity , which means that they are not useful for general-purpose primality testing , but they are often quite powerful when the tested number n is known to have a special form 